{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-mle-optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgo5GU/i+yriYnjo97pDfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DepartmentOfStatisticsPUE/bi-2021/blob/main/materials/3_mle_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7QawdiVUACk"
      },
      "source": [
        "# Metoda największej wiarygodności\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6KiRA1uUD9H"
      },
      "source": [
        "\n",
        "## Teoria\n",
        "\n",
        "If $f_{i}\\left(k_{i} ; \\mathbf{\\theta}\\right)$ is the PDF of a random-variable  where  $\\mathbf{\\theta}$ is a vector of parameters (e.g. $\\lambda$ in Poisson distribution), then for a collection of $N$ independent samples from this distribution, the joint distribution the random vector  $k_i$ is\n",
        "\n",
        "\\begin{equation}\n",
        "        f(\\mathbf{k} ; \\mathbf{\\theta})=\\prod_{i=1}^{N} f_{i}\\left(k_{i} ; \\mathbf{\\theta}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The maximum likelihood estimate of the parameters  $\\mathbf{\\theta}$ are the parameters which maximize this function with $\\mathbf{x}$ fixed and given by the data:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\hat{\\mathbf{\\theta}} =\\arg \\max _{\\mathbf{\\theta}} f(\\mathbf{k} ; \\mathbf{\\theta}) =\\arg \\min _{\\theta} l_{\\mathbf{k}}(\\mathbf{\\theta}),\n",
        "\\end{equation}\n",
        "\n",
        "where \n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "    l_{\\mathrm{k}}(\\mathbf{\\theta}) =-\\sum_{i=1}^{N} \\log f\\left(k_{i} ; \\mathbf{\\theta}\\right) =-N \\log f\\left(k_{i} ; \\mathbf{\\theta}\\right) \n",
        "\\end{equation}\n",
        "\n",
        "## Maximum likelihood -- przykład dla rozkładu Poissona\n",
        "\n",
        "\n",
        "Likelihood function for Poisson distribution is\n",
        "\n",
        "\\begin{equation}\n",
        "        L\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=\\prod_{j=1}^{n} \\exp (-\\lambda) \\frac{1}{x_{j} !} \\lambda^{x_{j}}\n",
        "\\end{equation}\n",
        "\n",
        "The log-likelihood function is\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "        l\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=-n \\lambda-\\sum_{j=1}^{n} \\ln \\left(x_{j} !\\right)+\\ln (\\lambda) \\sum_{j=1}^{n} x_{j}\n",
        "\\end{equation}\n",
        "\n",
        "The maximum likelihood estimator of $\\lambda$\n",
        "\n",
        "\\begin{equation}\n",
        "        \\hat{\\lambda}=\\frac{1}{n} \\sum_{j=1}^{n} x_{j}\n",
        "\\end{equation}\n",
        "\n",
        "## Maximum likelihood -- practice\n",
        "\n",
        "+ Depending on the distribution it is possible to derive closed form for the parameters $\\mathbf{\\theta}$.\n",
        "+ In other cases, numerical methods that require gradient (first derivatives) and hessian (second derivatives) are used, for example Newton-Raphson method:\n",
        "+ Statistical packages also implement derivative-free optimization methods that do not require to calculate gradient and hessian.\n",
        "\n",
        "\n",
        "## Newton's method -- single parameter function\n",
        "\n",
        "Let $f$: $\\mathbb{R}^1 \\to \\mathbb{R}^1 $ be a differentiable function. We seek a solution of $f(x)=0$, starting from an initial estimate $x_0=x_1$. \n",
        "    \n",
        "At the $n$'s step, given $x_n$,  compute the next approximation $x_{n+1}$ by\n",
        "    \n",
        "\\begin{equation}\n",
        "        x_{n+1} = x_{n} - \\frac{f(x_n)}{f'(x_n)}\n",
        "\\end{equation} \n",
        "    \n",
        "and repeat until converge (i.e. $|x_{n+1}-x_n| < \\epsilon$).\n",
        "    \n",
        "For step by step examples see: http://amsi.org.au/ESA_Senior_Years/SeniorTopic3/3j/3j_2content_2.html.\n",
        "\n",
        "\n",
        "## Newton-Rapshon algorithm -- multivariate case\n",
        "\n",
        " Newton's method for optimization consists of applying Newton's method for solving systems of equations, where the equations are the first order conditions, saying that the gradient should equal the zero vector.\n",
        "    \n",
        "\\begin{equation}\n",
        "        \\nabla \\mathbf{f}(\\mathbf{x}) = \\mathbf{0}\n",
        "\\end{equation}\n",
        "    \n",
        "A second order Taylor expansion of the left-hand side leads to the iterative scheme\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf{x}_{n+1} = \\mathbf{x}_{n} - \\mathbf{H}(\\mathbf{x}_{n})^{-1}\\nabla \\mathbf{f}(\\mathbf{x}_{n}),\n",
        "\\end{equation}\n",
        "\n",
        "where $\\nabla \\mathbf{f}(\\mathbf{x}_{n})$ is a gradient and $\\mathbf{H}(\\mathbf{x}_{n})$ is a hessian of $\\mathbf{f}(\\mathbf{x}_{n})$ (second order derivatives). \n",
        "\n",
        "## Optimization procedures\n",
        "\n",
        "There are plenty of different optimisation procedures that may be used for estimating parameters:\n",
        "\n",
        " + Gradient free\n",
        "\n",
        "    + Nelder-Mead\n",
        "    + Simulated Annealing\n",
        "    + Particle Swarm\n",
        "\n",
        "+ Gradient required\n",
        "    + Conjugate Gradient Descent\n",
        "    + Gradient Descent\n",
        "    + (L-)BFGS\n",
        "\n",
        "+ Hessian required\n",
        "    + Newton's Method\n",
        "    + Newton's Method With a Trust Region\n",
        "\n",
        "For more, see **Optim.jl** documentation https://julianlsolvers.github.io/Optim.jl or **scipy.optimize** https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvg8XDyVcLNc"
      },
      "source": [
        "# Przykład #1 \n",
        " \n",
        "## Zadanie\n",
        "\n",
        "Assume that $X$ follows zero-truncated Poisson distribution given by\n",
        "\n",
        "$$\n",
        "P(X=x, X>0; \\lambda) = \\frac{f(x; \\lambda)}{1 - f(0;\\lambda)} = \\frac{\\lambda^x e^{-\\lambda}}{x!(1-e^{-\\lambda})} =  \\frac{\\lambda^x}{(e^\\lambda-1)x!},\n",
        "$$\n",
        "\n",
        "complete the following task:\n",
        "\n",
        "+ estimate $\\lambda$ based on the maximum likelihood estimation method.\n",
        "+ Note that, obtaining $\\lambda$ requires deriving log-likelihood function (also gradient with respect to $\\lambda$) and applying optimization procedure (e.g. \\texttt{optim} function).\n",
        "\n",
        "\n",
        "## Rozwiązanie\n",
        "\n",
        "We start with likelihood function\n",
        "\n",
        "$$\n",
        "L = \\prod_i \\frac{\\lambda^x_i}{(e^\\lambda-1)x_i!},\n",
        "$$\n",
        "\n",
        "then we compute log-likelihood\n",
        "\n",
        "$$\n",
        "    \\log L = \\log\n",
        "    \\left(\n",
        "    \\prod_i \\frac{\\lambda^x_i}{(e^\\lambda-1)x_i!}\n",
        "    \\right) = \n",
        "    \\sum_i \\log \n",
        "    \\left( \n",
        "    \\frac{\\lambda^x_i}{(e^\\lambda-1)x_i!}\n",
        "    \\right)\n",
        "$$ \n",
        "\n",
        "after simplification we get\n",
        "\n",
        "$$\n",
        "\\log L = \\sum_i x_i \\log \\lambda - \\sum_i \\log(e^\\lambda-1) - \\sum_i \\log(x_i!) \n",
        "$$ \n",
        "\n",
        "In order to get estimate of $\\lambda$ we need to calculate derivatives with respect to this parameter. Thus, gradient is given by \n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\log L}{\\partial \\lambda} = \\frac{\\sum_i x_i}{\\lambda} - \\frac{n e^\\lambda}{e^\\lambda - 1} = \\frac{\\sum_i x_i}{\\lambda} - n \\frac{e^\\lambda}{e^\\lambda - 1}. \n",
        "$$\n",
        "\n",
        "We can also calculate second derivative (hessian)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial^2 \\log L}{\\partial \\lambda^2} =  - \\frac{\\sum_i x_i}{\\lambda^2} + n \\frac{e^\\lambda}{(e^\\lambda-1)^2}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0bIYVBUE0r"
      },
      "source": [
        "# Implementacja w R\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaEzTfUJTyh7",
        "outputId": "0b2020d8-165a-4da5-f2bb-f87d6b747d39"
      },
      "source": [
        "install.packages(c(\"rootSolve\", \"maxLik\", \"extraDistr\", \"numDeriv\", \"rbenchmark\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kswvi5MY4E7"
      },
      "source": [
        "library(maxLik)\n",
        "library(rootSolve)\n",
        "library(extraDistr) ## rtpois\n",
        "library(numDeriv) ## numerical gradient\n",
        "library(rbenchmark)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kWMQ9E6cygP"
      },
      "source": [
        "ll <- function(par, x) {\n",
        "  m <- sum(x)*log(par)-length(x)*log(exp(par)-1)\n",
        "  m\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEPBb2jXcz-2"
      },
      "source": [
        "grad <- function(par, x)  {\n",
        "  g <- sum(x) / par - length(x)*exp(par)/(exp(par)-1)\n",
        "  g\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7U2dV8Wc2Tb"
      },
      "source": [
        "hess <- function(par, x) {\n",
        "  h <- -sum(x)/par^2 + length(x)*exp(par)/(exp(par)-1)^2 \n",
        "  h\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnyE2z6mc3s9"
      },
      "source": [
        "set.seed(123)\n",
        "x <- rtpois(10000, lambda = 2.5, a = 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Faki4ccNc5Xf",
        "outputId": "03b3caee-c869-4366-f73b-bc068dfb1223"
      },
      "source": [
        "res <- maxLik(logLik = ll, start = 1, x = x, method = \"NR\")\n",
        "summary(res)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------------------------\n",
              "Maximum Likelihood estimation\n",
              "Newton-Raphson maximisation, 6 iterations\n",
              "Return code 2: successive function values within tolerance limit (tol)\n",
              "Log-Likelihood: 637.5431 \n",
              "1  free parameters\n",
              "Estimates:\n",
              "     Estimate Std. error t value Pr(> t)    \n",
              "[1,]  2.47732    0.01714   144.6  <2e-16 ***\n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "--------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "BT7BeCUjc7uu",
        "outputId": "b460b917-7dd6-4c1a-f818-be1009da3b5f"
      },
      "source": [
        "res2 <- maxLik(logLik = ll,  grad = grad, hess = hess, start = 1, x = x, method = \"NR\")\n",
        "summary(res2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------------------------\n",
              "Maximum Likelihood estimation\n",
              "Newton-Raphson maximisation, 6 iterations\n",
              "Return code 1: gradient close to zero (gradtol)\n",
              "Log-Likelihood: 637.5431 \n",
              "1  free parameters\n",
              "Estimates:\n",
              "     Estimate Std. error t value Pr(> t)    \n",
              "[1,]  2.47732    0.01713   144.6  <2e-16 ***\n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "--------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12KLKJ2TgdMU"
      },
      "source": [
        "## Przykład 2\n",
        "\n",
        "Assume that $X$ follows Poisson distribution given by \n",
        "\n",
        "$$\n",
        "P(X=x, \\lambda_i) = \\frac{\\lambda^x e^{-\\lambda}}{x!},\n",
        "$$\n",
        "\n",
        "where $\\lambda_i = \\theta_0 + \\theta_1 \\times z_i$, $\\theta_0=0.5$, $\\theta_1=0.5$, and $z_i \\sim \\text{Bern}(0.7)$ and number of observations is equal to $n=10,000$.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "+ generate $z_i$,\n",
        "+ generate $\\lambda_i$ according to $\\theta_0 + \\theta_1 \\times z_i$,\n",
        "+ generate $X \\sim Poisson(\\lambda_i)$\n",
        "+ derive log-likelihood, gradient and hessian,\n",
        "+ obtain MLE of $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1)$ using Newton-Raphson method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "cIL9x3dGgrca",
        "outputId": "d10e0ef9-26a4-47c2-bb4d-7c51ee7fc592"
      },
      "source": [
        "set.seed(123)\n",
        "n <- 10000\n",
        "z <- rbinom(n = n, prob = 0.7, size = 1)\n",
        "theta_true <- c(1, 1)\n",
        "lambda_true <- theta_true[1] + theta_true[2]*z\n",
        "X <- rpois(n = n, lambda = lambda_true)\n",
        "table(X)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "X\n",
              "   0    1    2    3    4    5    6    7    8    9 \n",
              "2069 2982 2441 1494  629  277   78   25    3    2 "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2qodlfZna5E"
      },
      "source": [
        "Funkcja log-wiarygodności\n",
        "\n",
        "$$\n",
        "logLik(\\theta_0, \\theta_1; X_i, z_i) = -\\lambda_i + x_i \\log(\\lambda_i) = -(\\theta_0 + \\theta_1z_i) + x_i \\log(\\theta_0 + \\theta_1z_i)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI7fxBrxhEmN"
      },
      "source": [
        "ll <- function(theta, z, X) {\n",
        "  \n",
        "  lam <- theta[1]+theta[2]*z\n",
        "  l <- X*log(lam) - lam\n",
        "  return(sum(l))\n",
        "}"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGLyP-YUz0cR"
      },
      "source": [
        "Gradient \n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\log L}{\\partial \\mathbf{\\theta}} = \n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial \\log L}{\\partial \\theta_0} & = \\frac{x_i}{\\theta_0 + \\theta_1 z_i} - 1\\\\\n",
        "\\frac{\\partial \\log L}{\\partial \\theta_1} & = \\frac{x_i z_i}{\\theta_0 + \\theta_1 z_i} - z_i\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfOj8Kph0PND"
      },
      "source": [
        "ll_grad <- function(theta, z, X) {\n",
        "  \n",
        "  lam <- theta[1]+theta[2]*z\n",
        "  l_g <- matrix(0, nrow = NROW(lam), ncol = 2)\n",
        "  l_g[,1] <- X/lam - 1\n",
        "  l_g[,2] <- X*z/lam - z\n",
        "  return(colSums(l_g))\n",
        "}"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tps_JQj21gq"
      },
      "source": [
        "Hessian\n",
        "\n",
        "$$\n",
        "\\frac{\\partial^2 \\log L}{\\partial \\mathbf{\\theta}^2} = \n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial^2 \\log L}{\\partial \\theta_0^2} & \\frac{\\partial^2 \\log L}{\\partial \\theta_0 \\partial\\theta_1} \\\\\n",
        "\\frac{\\partial^2 \\log L}{\\partial \\theta_0 \\partial\\theta_1} & \\frac{\\partial^2 \\log L}{\\partial \\theta_1^2} \\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "\\sum_i \\frac{-x_i}{(\\theta_0+ \\theta_1 z_i)^2} & \\sum_i \\frac{-x_i z_i}{(\\theta_0+ \\theta_1 z_i)^2} \\\\\n",
        "\\sum_i \\frac{-x_i z_i}{(\\theta_0+ \\theta_1 z_i)^2} & \\sum_i \\frac{-x_i z_i^2}{(\\theta_0+ \\theta_1 z_i)^2} \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqfA4RjW43tf"
      },
      "source": [
        "ll_hess <- function(theta, z, X) {\n",
        "  \n",
        "  lam <- theta[1]+theta[2]*z\n",
        "  l_h <- matrix(0, nrow = 2, ncol = 2)\n",
        "  l_h[1,1] <- sum(-X / lam^2)\n",
        "  l_h[2,2] <- sum(-X * z^2 / lam^2)\n",
        "  l_h[1,2] <- l_h[2,1] <- sum(-X * z/ lam^2)\n",
        "  return(l_h)\n",
        "}"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "tsNZIb2NhVYA",
        "outputId": "3faf17c1-919e-4a7f-edb3-d3a39a40d4d6"
      },
      "source": [
        "solution <- maxLik(logLik = ll, start = c(1,1), z = z, X = X, method = \"NR\")\n",
        "summary(solution)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------------------------\n",
              "Maximum Likelihood estimation\n",
              "Newton-Raphson maximisation, 3 iterations\n",
              "Return code 2: successive function values within tolerance limit (tol)\n",
              "Log-Likelihood: -7291.117 \n",
              "2  free parameters\n",
              "Estimates:\n",
              "     Estimate Std. error t value Pr(> t)    \n",
              "[1,]  0.96816    0.01811   53.46  <2e-16 ***\n",
              "[2,]  1.02872    0.02472   41.61  <2e-16 ***\n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "--------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "gNIBviEW1V4d",
        "outputId": "5017cdae-58ef-447d-c8a9-ad68be46ef53"
      },
      "source": [
        "solution <- maxLik(logLik = ll, grad =  ll_grad, hess = ll_hess, start = c(1,1), z = z, X = X, method = \"NR\")\n",
        "summary(solution)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------------------------\n",
              "Maximum Likelihood estimation\n",
              "Newton-Raphson maximisation, 3 iterations\n",
              "Return code 1: gradient close to zero (gradtol)\n",
              "Log-Likelihood: -7291.117 \n",
              "2  free parameters\n",
              "Estimates:\n",
              "     Estimate Std. error t value Pr(> t)    \n",
              "[1,]  0.96816    0.01811   53.46  <2e-16 ***\n",
              "[2,]  1.02872    0.02472   41.61  <2e-16 ***\n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "--------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-wezmu_2ZUY"
      },
      "source": [
        "Porównajmy jakie wartości uzyskujemy z gradientu i hessianu wyznaczonego analitycznie, a jakie z wyznaczonego numerycznie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "9luGz1R-2izW",
        "outputId": "704df84c-f3d2-4d9a-ccf3-20b46eec3b44"
      },
      "source": [
        "ll_grad(c(1,1), z=z, X=X)\n",
        "numDeriv::grad(ll, c(1,1), z=z, X= X)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] -105  -11"
            ],
            "text/latex": "\\begin{enumerate*}\n\\item -105\n\\item -11\n\\end{enumerate*}\n",
            "text/markdown": "1. -105\n2. -11\n\n\n",
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>-105</li><li>-11</li></ol>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] -105  -11"
            ],
            "text/latex": "\\begin{enumerate*}\n\\item -105.000000045473\n\\item -11.0000000564337\n\\end{enumerate*}\n",
            "text/markdown": "1. -105.000000045473\n2. -11.0000000564337\n\n\n",
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>-105.000000045473</li><li>-11.0000000564337</li></ol>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "kB0I4MUv5Ojn",
        "outputId": "e7ece609-6592-4d6e-c801-a8317381736c"
      },
      "source": [
        "ll_hess(c(1,1), z=z, X=X)\n",
        "numDeriv::hessian(ll, c(1,1), z=z, X=X)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     [,1]    [,2]   \n",
              "[1,] -6376.5 -3518.5\n",
              "[2,] -3518.5 -3518.5"
            ],
            "text/latex": "A matrix: 2 × 2 of type dbl\n\\begin{tabular}{ll}\n\t -6376.5 & -3518.5\\\\\n\t -3518.5 & -3518.5\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA matrix: 2 × 2 of type dbl\n\n| -6376.5 | -3518.5 |\n| -3518.5 | -3518.5 |\n\n",
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 2 of type dbl</caption>\n",
              "<tbody>\n",
              "\t<tr><td>-6376.5</td><td>-3518.5</td></tr>\n",
              "\t<tr><td>-3518.5</td><td>-3518.5</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     [,1]    [,2]   \n",
              "[1,] -6376.5 -3518.5\n",
              "[2,] -3518.5 -3518.5"
            ],
            "text/latex": "A matrix: 2 × 2 of type dbl\n\\begin{tabular}{ll}\n\t -6376.5 & -3518.5\\\\\n\t -3518.5 & -3518.5\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA matrix: 2 × 2 of type dbl\n\n| -6376.5 | -3518.5 |\n| -3518.5 | -3518.5 |\n\n",
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 2 of type dbl</caption>\n",
              "<tbody>\n",
              "\t<tr><td>-6376.5</td><td>-3518.5</td></tr>\n",
              "\t<tr><td>-3518.5</td><td>-3518.5</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enIfpCmq569C"
      },
      "source": [
        "Porównamy szybkość działania"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNCAZw006pND"
      },
      "source": [
        "## definiujemy sobie funkcje wcześniej\n",
        "ll_grad_numeric <- function(theta, z, X) {\n",
        "  numDeriv::grad(ll, theta, z=z, X=X)\n",
        "}\n",
        "\n",
        "ll_hess_numeric <- function(theta, z, X) {\n",
        "  numDeriv::hessian(ll, theta, z=z, X=X)\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "2-TAGXHd6Gv8",
        "outputId": "17934a33-2d5d-4a35-c9db-6f64a6fa9a84"
      },
      "source": [
        "benchmark(maxlik_bez = maxLik(logLik = ll, start = c(1,1), z = z, X = X, method = \"NR\"), \n",
        "          maxlik_analytic = maxLik(logLik = ll, grad =  ll_grad, hess = ll_hess, start = c(1,1), z = z, X = X, method = \"NR\"),\n",
        "          maxlik_numerical = maxLik(logLik = ll, grad =  ll_grad_numeric, hess = ll_hess_numeric, start = c(1,1), z = z, X = X, method = \"NR\"))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  test             replications elapsed relative user.self sys.self user.child\n",
              "2 maxlik_analytic  100          0.998   1.000    0.978     0.017    0         \n",
              "1 maxlik_bez       100          4.496   4.505    4.403     0.070    0         \n",
              "3 maxlik_numerical 100          9.165   9.183    9.023     0.118    0         \n",
              "  sys.child\n",
              "2 0        \n",
              "1 0        \n",
              "3 0        "
            ],
            "text/latex": "A data.frame: 3 × 8\n\\begin{tabular}{r|llllllll}\n  & test & replications & elapsed & relative & user.self & sys.self & user.child & sys.child\\\\\n  & <chr> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t2 & maxlik\\_analytic  & 100 & 0.998 & 1.000 & 0.978 & 0.017 & 0 & 0\\\\\n\t1 & maxlik\\_bez       & 100 & 4.496 & 4.505 & 4.403 & 0.070 & 0 & 0\\\\\n\t3 & maxlik\\_numerical & 100 & 9.165 & 9.183 & 9.023 & 0.118 & 0 & 0\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA data.frame: 3 × 8\n\n| <!--/--> | test &lt;chr&gt; | replications &lt;int&gt; | elapsed &lt;dbl&gt; | relative &lt;dbl&gt; | user.self &lt;dbl&gt; | sys.self &lt;dbl&gt; | user.child &lt;dbl&gt; | sys.child &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|\n| 2 | maxlik_analytic  | 100 | 0.998 | 1.000 | 0.978 | 0.017 | 0 | 0 |\n| 1 | maxlik_bez       | 100 | 4.496 | 4.505 | 4.403 | 0.070 | 0 | 0 |\n| 3 | maxlik_numerical | 100 | 9.165 | 9.183 | 9.023 | 0.118 | 0 | 0 |\n\n",
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 3 × 8</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>test</th><th scope=col>replications</th><th scope=col>elapsed</th><th scope=col>relative</th><th scope=col>user.self</th><th scope=col>sys.self</th><th scope=col>user.child</th><th scope=col>sys.child</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>2</th><td>maxlik_analytic </td><td>100</td><td>0.998</td><td>1.000</td><td>0.978</td><td>0.017</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>1</th><td>maxlik_bez      </td><td>100</td><td>4.496</td><td>4.505</td><td>4.403</td><td>0.070</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>maxlik_numerical</td><td>100</td><td>9.165</td><td>9.183</td><td>9.023</td><td>0.118</td><td>0</td><td>0</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}