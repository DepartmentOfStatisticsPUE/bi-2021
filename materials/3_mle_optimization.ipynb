{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-mle-optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOk33T/r1OV7xtKjkq6H/Sm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DepartmentOfStatisticsPUE/bi-2021/blob/main/materials/3_mle_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7QawdiVUACk"
      },
      "source": [
        "# Metoda największej wiarygodności\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6KiRA1uUD9H"
      },
      "source": [
        "\n",
        "## Teoria\n",
        "\n",
        "If $f_{i}\\left(k_{i} ; \\mathbf{\\theta}\\right)$ is the PDF of a random-variable  where  $\\mathbf{\\theta}$ is a vector of parameters (e.g. $\\lambda$ in Poisson distribution), then for a collection of $N$ independent samples from this distribution, the joint distribution the random vector  $k_i$ is\n",
        "\n",
        "\\begin{equation}\n",
        "        f(\\mathbf{k} ; \\mathbf{\\theta})=\\prod_{i=1}^{N} f_{i}\\left(k_{i} ; \\mathbf{\\theta}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The maximum likelihood estimate of the parameters  $\\mathbf{\\theta}$ are the parameters which maximize this function with $\\mathbf{x}$ fixed and given by the data:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\hat{\\mathbf{\\theta}} =\\arg \\max _{\\mathbf{\\theta}} f(\\mathbf{k} ; \\mathbf{\\theta}) =\\arg \\min _{\\theta} l_{\\mathbf{k}}(\\mathbf{\\theta}),\n",
        "\\end{equation}\n",
        "\n",
        "where \n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "    l_{\\mathrm{k}}(\\mathbf{\\theta}) =-\\sum_{i=1}^{N} \\log f\\left(k_{i} ; \\mathbf{\\theta}\\right) =-N \\log f\\left(k_{i} ; \\mathbf{\\theta}\\right) \n",
        "\\end{equation}\n",
        "\n",
        "## Maximum likelihood -- przykład dla rozkładu Poissona\n",
        "\n",
        "\n",
        "Likelihood function for Poisson distribution is\n",
        "\n",
        "\\begin{equation}\n",
        "        L\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=\\prod_{j=1}^{n} \\exp (-\\lambda) \\frac{1}{x_{j} !} \\lambda^{x_{j}}\n",
        "\\end{equation}\n",
        "\n",
        "The log-likelihood function is\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "        l\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=-n \\lambda-\\sum_{j=1}^{n} \\ln \\left(x_{j} !\\right)+\\ln (\\lambda) \\sum_{j=1}^{n} x_{j}\n",
        "\\end{equation}\n",
        "\n",
        "The maximum likelihood estimator of $\\lambda$\n",
        "\n",
        "\\begin{equation}\n",
        "        \\hat{\\lambda}=\\frac{1}{n} \\sum_{j=1}^{n} x_{j}\n",
        "\\end{equation}\n",
        "\n",
        "## Maximum likelihood -- practice\n",
        "\n",
        "+ Depending on the distribution it is possible to derive closed form for the parameters $\\mathbf{\\theta}$.\n",
        "+ In other cases, numerical methods that require gradient (first derivatives) and hessian (second derivatives) are used, for example Newton-Raphson method:\n",
        "+ Statistical packages also implement derivative-free optimization methods that do not require to calculate gradient and hessian.\n",
        "\n",
        "\n",
        "## Newton's method -- single parameter function\n",
        "\n",
        "Let $f$: $\\mathbb{R}^1 \\to \\mathbb{R}^1 $ be a differentiable function. We seek a solution of $f(x)=0$, starting from an initial estimate $x_0=x_1$. \n",
        "    \n",
        "At the $n$'s step, given $x_n$,  compute the next approximation $x_{n+1}$ by\n",
        "    \n",
        "\\begin{equation}\n",
        "        x_{n+1} = x_{n} - \\frac{f(x_n)}{f'(x_n)}\n",
        "\\end{equation} \n",
        "    \n",
        "and repeat until converge (i.e. $|x_{n+1}-x_n| < \\epsilon$).\n",
        "    \n",
        "For step by step examples see: http://amsi.org.au/ESA_Senior_Years/SeniorTopic3/3j/3j_2content_2.html.\n",
        "\n",
        "\n",
        "## Newton-Rapshon algorithm -- multivariate case\n",
        "\n",
        " Newton's method for optimization consists of applying Newton's method for solving systems of equations, where the equations are the first order conditions, saying that the gradient should equal the zero vector.\n",
        "    \n",
        "\\begin{equation}\n",
        "        \\nabla \\mathbf{f}(\\mathbf{x}) = \\mathbf{0}\n",
        "\\end{equation}\n",
        "    \n",
        "A second order Taylor expansion of the left-hand side leads to the iterative scheme\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf{x}_{n+1} = \\mathbf{x}_{n} - \\mathbf{H}(\\mathbf{x}_{n})^{-1}\\nabla \\mathbf{f}(\\mathbf{x}_{n}),\n",
        "\\end{equation}\n",
        "\n",
        "where $\\nabla \\mathbf{f}(\\mathbf{x}_{n})$ is a gradient and $\\mathbf{H}(\\mathbf{x}_{n})$ is a hessian of $\\mathbf{f}(\\mathbf{x}_{n})$ (second order derivatives). \n",
        "\n",
        "## Optimization procedures\n",
        "\n",
        "There are plenty of different optimisation procedures that may be used for estimating parameters:\n",
        "\n",
        " + Gradient free\n",
        "\n",
        "    + Nelder-Mead\n",
        "    + Simulated Annealing\n",
        "    + Particle Swarm\n",
        "\n",
        "+ Gradient required\n",
        "    + Conjugate Gradient Descent\n",
        "    + Gradient Descent\n",
        "    + (L-)BFGS\n",
        "\n",
        "+ Hessian required\n",
        "    + Newton's Method\n",
        "    + Newton's Method With a Trust Region\n",
        "\n",
        "For more, see **Optim.jl** documentation https://julianlsolvers.github.io/Optim.jl or **scipy.optimize** https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvg8XDyVcLNc"
      },
      "source": [
        "# Practice -- exercise\n",
        "\n",
        "## Task \n",
        "\n",
        "Assume that $X$ follows zero-truncated Poisson distribution given by\n",
        "\n",
        "$$\n",
        "P(X=x, X>0; \\lambda) = \\frac{f(x; \\lambda)}{1 - f(0;\\lambda)} = \\frac{\\lambda^x e^{-\\lambda}}{x!(1-e^{-\\lambda})} =  \\frac{\\lambda^x}{(e^\\lambda-1)x!},\n",
        "$$\n",
        "\n",
        "complete the following task:\n",
        "\n",
        "+ estimate $\\lambda$ based on the maximum likelihood estimation method.\n",
        "+ Note that, obtaining $\\lambda$ requires deriving log-likelihood function (also gradient with respect to $\\lambda$) and applying optimization procedure (e.g. \\texttt{optim} function).\n",
        "\n",
        "\n",
        "## Solution\n",
        "\n",
        "We start with likelihood function\n",
        "\n",
        "$$\n",
        "L = \\prod_i \\frac{\\lambda^x_i}{(e^\\lambda-1)x_i!},\n",
        "$$\n",
        "\n",
        "then we compute log-likelihood\n",
        "\n",
        "$$\n",
        "    \\log L = \\log\n",
        "    \\left(\n",
        "    \\prod_i \\frac{\\lambda^x_i}{(e^\\lambda-1)x_i!}\n",
        "    \\right) = \n",
        "    \\sum_i \\log \n",
        "    \\left( \n",
        "    \\frac{\\lambda^x_i}{(e^\\lambda-1)x_i!}\n",
        "    \\right)\n",
        "$$ \n",
        "\n",
        "after simplification we get\n",
        "\n",
        "$$\n",
        "\\log L = \\sum_i x_i \\log \\lambda - \\sum_i \\log(e^\\lambda-1) - \\sum_i \\log(x_i!) \n",
        "$$ \n",
        "\n",
        "In order to get estimate of $\\lambda$ we need to calculate derivatives with respect to this parameter. Thus, gradient is given by \n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\log L}{\\partial \\lambda} = \\frac{\\sum_i x_i}{\\lambda} - \\frac{n e^\\lambda}{e^\\lambda - 1} = \\frac{\\sum_i x_i}{\\lambda} - n \\frac{e^\\lambda}{e^\\lambda - 1}. \n",
        "$$\n",
        "\n",
        "We can also calculate second derivative (hessian)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial^2 \\log L}{\\partial \\lambda^2} =  - \\frac{\\sum_i x_i}{\\lambda^2} + n \\frac{e^\\lambda}{(e^\\lambda-1)^2}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0bIYVBUE0r"
      },
      "source": [
        "# Implementacja w R\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaEzTfUJTyh7",
        "outputId": "0b2020d8-165a-4da5-f2bb-f87d6b747d39"
      },
      "source": [
        "install.packages(c(\"rootSolve\", \"maxLik\", \"extraDistr\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kswvi5MY4E7",
        "outputId": "0e241a4c-b2ae-4578-e141-0f612a3c9633"
      },
      "source": [
        "library(maxLik)\n",
        "library(rootSolve)\n",
        "library(extraDistr) ## rtpois"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Attaching package: ‘rootSolve’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:maxLik’:\n",
            "\n",
            "    gradient, hessian\n",
            "\n",
            "\n",
            "\n",
            "Attaching package: ‘extraDistr’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:miscTools’:\n",
            "\n",
            "    ddnorm\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kWMQ9E6cygP"
      },
      "source": [
        "ll <- function(par, x) {\n",
        "  m <- sum(x)*log(par)-length(x)*log(exp(par)-1)\n",
        "  m\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEPBb2jXcz-2"
      },
      "source": [
        "grad <- function(par, x)  {\n",
        "  g <- sum(x) / par - length(x)*exp(par)/(exp(par)-1)\n",
        "  g\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7U2dV8Wc2Tb"
      },
      "source": [
        "hess <- function(par, x) {\n",
        "  h <- -sum(x)/par^2 + length(x)*exp(par)/(exp(par)-1)^2 \n",
        "  h\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnyE2z6mc3s9"
      },
      "source": [
        "set.seed(123)\n",
        "x <- rtpois(10000, lambda = 2.5, a = 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Faki4ccNc5Xf",
        "outputId": "03b3caee-c869-4366-f73b-bc068dfb1223"
      },
      "source": [
        "res <- maxLik(logLik = ll, start = 1, x = x, method = \"NR\")\n",
        "summary(res)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------------------------\n",
              "Maximum Likelihood estimation\n",
              "Newton-Raphson maximisation, 6 iterations\n",
              "Return code 2: successive function values within tolerance limit (tol)\n",
              "Log-Likelihood: 637.5431 \n",
              "1  free parameters\n",
              "Estimates:\n",
              "     Estimate Std. error t value Pr(> t)    \n",
              "[1,]  2.47732    0.01714   144.6  <2e-16 ***\n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "--------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "BT7BeCUjc7uu",
        "outputId": "b460b917-7dd6-4c1a-f818-be1009da3b5f"
      },
      "source": [
        "res2 <- maxLik(logLik = ll,  grad = grad, hess = hess, start = 1, x = x, method = \"NR\")\n",
        "summary(res2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------------------------\n",
              "Maximum Likelihood estimation\n",
              "Newton-Raphson maximisation, 6 iterations\n",
              "Return code 1: gradient close to zero (gradtol)\n",
              "Log-Likelihood: 637.5431 \n",
              "1  free parameters\n",
              "Estimates:\n",
              "     Estimate Std. error t value Pr(> t)    \n",
              "[1,]  2.47732    0.01713   144.6  <2e-16 ***\n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "--------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}